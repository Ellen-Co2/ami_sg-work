#treat train------split for device count in different status, parse time to mins,hrs,wksfrom pandas import DataFrame, Seriesimport numpy as npimport pandas as pdfrom datetime import datetime, timedeltaimport sqlite3from sqlalchemy import create_enginedb_add = str(raw_input('please enter the full directory for the stored database: '))db_name = str(raw_input('please enter the full name for the stored database: '))try:    s = 'sqlite:///' + db_add + db_name    disk_engine = create_engine(s) except:    print('not correct path, using default: %s') % 'sqlite:////Users/AAA218/Desktop/amifull_sg/new.sqlite'    #conn = sqlite3.connect('new.sqlite')    disk_engine = create_engine('sqlite:////Users/AAA218/Desktop/amifull_sg/new.sqlite'')print "start read sql.."begi = datetime.now()# process to get UNKNOWN & ASSOCIATED status numberAll = pd.read_sql_query('SELECT mac,bldid, sgtime, status ,interv FROM newwifi ',disk_engine)#unkn = pd.read_sql_query('SELECT mac,bldid, sgtime FROM wifi WHERE status = "UNKNOWN"',disk_engine)#ass = pd.read_sql_query('SELECT mac,bldid, sgtime FROM wifi WHERE status = "ASSOCIATED" ',disk_engine)lost = pd.read_sql_query('SELECT mac FROM lost',disk_engine).mac.unique()unkn = All[All['status']=='UNKNOWN'].copy()ass = All[All['status']=='ASSOCIATED'].copy()poin1 = datetime.now()t1 = timedelta.total_seconds(poin1 - begi)print "spend %s reading " %t1samp["sgtime"] = pd.to_datetime(samp['sgtime'])start = samp.iloc[0,2]try:    t_interv = int(raw_input("please specify the time span in minutes for countting (or 'Enter' to use default 20 mins)"))    t = int(timedelta.total_seconds(samp.sgtime[1]-start)/t_interv)except:    print "unvalid value, using defaults: ", 20, 'mins'    t_interv = 20 *60    print "start make time group interval...fill the lost entries' interval nan"tt = []for i,row in enumerate(samp.values):    #if i <= 10:    t = int(timedelta.total_seconds(samp.sgtime[i]-start)/t_interv)        #print t    #else: break    if samp.mac[i] in lost:        temp = np.nan    else:        temp = samp.interval[i]    interv.append(temp)    tt.append(t)print "dims of t_grp: ",len(tt)poin2 = datetime.now()print "spend %s make t_grp " %timedelta.total_seconds(poin2 - poin1)samp["interv"] = intervsamp["t_grp"] = ttprint "now dimension is : " , All.shapeprint "columns are: " , All.columnsdef sta_num(df):    df["sgtime"] = pd.to_datetime(df["sgtime"])    tt = []    for i,row in enumerate(df.values):        #if i <= 10:        t = int(timedelta.total_seconds(df.sgtime[i]-start)/1200)            #print t        #else: break        tt.append(t)        #print tt    df["t_grp"] = tt    sta_agg = {"sgtime": 'min',"mac": lambda x: len(x.unique())}    df_agg = df.groupby(["bldid","t_grp"]).agg(sta_agg)    return df_agg#sta_num(prob).to_csv("prob_agg.csv",index = True)sta_num(ass).to_csv("ass_agg.csv", index = True)sta_num(unkn).to_csv("unknown_agg.csv",index = True)print "start aggregation....."## might need modify when more fields add in..aggre = {"sgtime": 'min', "mac": lambda x: len(x.unique()),"interv":'median' }All_ans = All.groupby(["bldid","t_grp"]).agg(aggre)All_ans.to_csv('all_agg.csv',index= True)print 'read in again...'pre_tr = pd.read_csv('/Users/AAA218/Desktop/amifull_sg/all_agg.csv',header = 0, sep =",",low_memory=False)print "new columns after agg are: \n", pre_tr.columnsprint " sample entries: \n", pre_tr.head()unk_new = pd.read_csv('/Users/AAA218/Desktop/amifull_sg/unknown_agg.csv',header = 0, sep =",",low_memory=False)ass_new = pd.read_csv('/Users/AAA218/Desktop/amifull_sg/ass_agg.csv',header = 0, sep =",",low_memory=False)unk_new.rename(columns = {'mac':'UNKNOWN'},inplace = True)ass_new.rename(columns = {'mac':'ASSOCIATED'},inplace = True)unk_new.drop("sgtime", axis = 1, inplace = True)ass_new.drop("sgtime", axis = 1, inplace = True)print "unknown to merge: \n", unk_new.head()print "associated to merge: \n", ass_new.head()print "start to join...."def comb(df1,df2):    df = pd.merge(df1,df2, how = "left", on = ['bldid','t_grp'])    print "null cases: \n", df.isnull().sum()    for col in df2.columns:        if col not in df1.columns:            #print col            df[col].fillna(0, inplace = True)    print "null check: \n", df.isnull().sum()    return dfa = comb(pre_tr, unk_new)a1 = comb(a, ass_new)a1["PROBING"] = a1["mac"]- a1["UNKNOWN"] - a1["ASSOCIATED"]print "---------sample:----------\n",a1.head()'''               mac              sgtimebldid   t_grp                         SDE1-02 0        1 2016-04-16 00:00:01        8        1 2016-04-16 02:40:01        23       1 2016-04-16 07:50:01        28       1 2016-04-16 09:35:02        29       1 2016-04-16 09:40:01'''hr = []mins = []weeks = []for i, row in enumerate(a1.values):    #if i >5 : break    tt = datetime.strptime(a1.sgtime[i], "%Y-%m-%d %H:%M:%S")    h = tt.hour    m = tt.minute    wk = tt.strftime("%w") #0-6    #print "hour:%s, minute: %s, day: %s " %(h ,m ,wk)    hr.append(h)    mins.append(m)    weeks.append(wk)temp = DataFrame({"hr":hr,"mins":mins,"wks":weeks})tr = pd.concat([a1,temp],axis=1)tr.rename(columns = {'mac':'device_num'},inplace = True)## make dummies for buildings#bldcode = pd.get_dummies(tr['bldid'])#tr = pd.concat([tr,bldcode],axis=1)#tr.drop("sgtime",axis = 1, inplace = True)print "final columns:\n ", tr.head()print "final dims: ",tr.shapetr.to_csv('tr_20min_named.csv',index = False)